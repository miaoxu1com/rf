python  数据存储  cvs格式存储 有自己特有的格式 就是用逗号分隔  然后是文本类型的可以使用编辑器 直接打开
execel  是二进制文件不能用文本编辑器直接打开  
config  文件只能保存二维的数据 不能在节点key下载再创建key value如果  [config]  language=name=en
那么name就会被识别为字符串  如果要使用就要在python中再次解析为 对象才行
dump  load  dumps   loads 就是 带s 是 str->dict  不带s的 就是文本到序列化为json的写文件数据
而且如果是关系型数据  就要用到关系型数据库 
如果非关系型数据可以用  文本文件类型存储  但是大量文件时还是要用数据库  如果数据就是一块一块的  没有关系 用redis存json 在服务端则可以减少数据的转换  mongon

如果输出的数据还需要后续 筛选查看 可能需要二次分析  用excel更合适  select过滤 query查询  


看资源管理器综合指标

命令行的输出一般都很有规律 比如 ping输出格式
来自 192.168.1.1 字节=32 ttl=55 可以把输出copy出来然后 放入excel进行 分列有的可能需要多次分列达到自己想要的  然后再排序这样就可以得到有规律的数据 筛选出可ping 的ip

excel可以直接选中一列数据然后插入  推荐的表格自动生成 透视表很有用 帮助分析
excel动态化图表可以用vb插件下拉框动态化改变 图表数据  主要是通过控件绑定数据实现的  为什么必须要控件因为控件是用来做 数据切换的的 excel中的引用数据就相当于一个变量是地址

既然你在内网中  内网一般都不会只有你一个所以可以用  内网工具扫描出 内网的局域网用户看那些局域网用户在线  找到了用户就可以扫描用户是否开启了服务找到服务 就进行攻防

如果是局域网扫描肯定都是有规律的  网段扫描  如果是外网 大部分场景是没有规律的数据  所以这个时候就不能用网段扫描了   只能把网段  用格式化化的文本  导入进行ping
大部分时间都是把没有规律的数据组装成有规律的数据



序列化就是格式化成字符串

还有就是字符串格式化  如果是 json 字符串格式化  里边有很多要替换的  为了保持json的结构 就不要用 + 进行拼接会很乱  这个时候可以用'''  但是有的是只是简单的字符串拼接并不是格式化的数据
直接用+连接就行
{
  "name":"zhagnsna"
  "age":['30']
}
'''

excel中文本后缀不会加0




多线程   高并发  就是集群开发 比如redis集群  数据库集群  niginx集群等等  这些就是高并发  页面越来越大 功能越来越复杂  一套监控系统  持续监控  评估预警页面性能  发现瓶颈 

还有就是一些任务适合什么数据库也要设计好  比如监控用redis  可以用缓存  因为监控  在很长一段时间 数据都不会更新 所以用key  把缓存的数据存入redis  减少数据库io  这就很合理  一种就是简单粗暴的
直接扩容  一种就是优化数据存储架构

对比数据直接对比整体就行了
把对比数据 和 返回的数据（返回的数据组装成和对比数据结构一致对比）



map(lambal )
filer  都是返回迭代对象需要转换成list

还有yaml  读取f.read  然后组织成yaml的数据库结构 写入yaml  f.write  然后调用加载yamlload把 文件序列加载到内存就是和dump  load一样的原理  然后yamldump把加载到内存中的对象转成json串
写入到文件

网上一些网站 视频采集网站也是用的  定时任务  所以我们不能用监控去做 视频采集的更新  监控只能有少数任务 而不适用于大量任务  而且要并行  同时监控采集但是 这样过于占用资源太过频繁  当任务过多时
就不适用  特定任务可以 
所以我们要分析   视频网站定时更新的时间是什么时候 怎么去研究   可以写一个监控某一个特定视频更新的时间点去推算  我们定时更新的时间点  我们定时更新的时间点一定要在  视频采集更新时间点之后


视频网站怎么知道增量更新了  肯定增加了list 判断list的长度或者 判断网页节点的长度 就能判断出来是否增加了  还有就是网站某一个点一定会定时更新  找到一定定时更新的点 这样我们就不用判断是否有更新再入库了
还有就是 爬取 会让你输入网页后帮你爬去内容  完结的需要怎么处理 没有完结的需要怎么处理  没有完结就
爬去是爬去的数据 而不是爬去页面

性能测试  web性能测试的概念 通过技术手段提升页面的访问速度  web指标  web性能测试的工具yslow 且工具会给出网站评分其实给出网站优化点就可以了   web性能测试的优化规则
web的访问过程  dns解析  建立连接  发送请求  服务器响应 接收数据 页面渲染（预处理->缓存查找时间 网络排队时间 cache）
优化规则 cache-control gzip  缓存 cdn等

不缓存过期的资源 合理设置缓存配置  过期  请求头一些事为了防止攻击带的

现在google都不支持 flash了 所以不要把视频输出成swf 格式了

内网扫描工具  扫描服务端口扫描出来了  在内网大部分都是ip直接访问 而不是域名所以扫出来ip肯定就可以访问ftp了拿ip直接访问ftp
ip  访问就要走7层协议



Python 分析网页应该用 requests + lxml，前者下载内容，后者有 HTML Parser 并且支持 XPath 语法。

标准库的 urllib 系的 API 设计实在反人类，Beautiful Soup 的 CSS 选择器其实远不如 XPath 灵活和易于修改



如何不刷新网页而监控网页变化？
我在用python监控一个网页 这个网页不定时的会更新 我要寻找需要匹配的关键词比如‘ABC’ 大概的程序框架如下
基本方法就是 用 selenium 获取源码 然后beautifulsoup解析 然后再去结果里面match 每2秒循环一次

while true:
    html = browser.page_source
    soup = BeautifulSoup(html)
    abc=soup.find_all(text=re.compile("(ABC)"))    
    if not abc:
         .....
    else:
         .....   
    browser.refresh()
    time.sleep(2.0 - ((time.time() - starttime) % 2.0))
现在问题就是这个程序很依赖网速，browser.refresh() 刷新一次有可能就会用1秒钟
有没有什么办法 不需要刷新网页 就能知道网页有变化
或者有没有其他办法能让我这个程序 不被网速拖累
